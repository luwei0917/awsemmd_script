{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "class RemoveFirstFrame(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, frame):\n",
    "        self.frame = frame\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.query(f\"Step % {frame} != 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression \n",
      " [[1555    6]\n",
      " [  35    4]]\n",
      "RandomForestClassifier \n",
      " [[1561    0]\n",
      " [  29   10]]\n",
      "SVC \n",
      " [[1557    4]\n",
      " [  33    6]]\n",
      "VotingClassifier \n",
      " [[1561    0]\n",
      " [  29   10]]\n",
      "1MBA\n",
      "[[1944    8]\n",
      " [  47    2]]\n",
      "T0766\n",
      "[[1946    6]\n",
      " [  45    4]]\n",
      "T0784\n",
      "[[1942   10]\n",
      " [  49    0]]\n",
      "T0792\n",
      "[[1948    4]\n",
      " [  43    6]]\n",
      "T0803\n",
      "[[1944    8]\n",
      " [  47    2]]\n",
      "T0815\n",
      "[[1946    6]\n",
      " [  45    4]]\n",
      "T0833\n",
      "[[1915    9]\n",
      " [  48    1]]\n",
      "T251\n",
      "[[1943    9]\n",
      " [  48    1]]\n",
      "p0.1_poly1\n"
     ]
    }
   ],
   "source": [
    "raw_test_data = pd.read_csv(\"/Users/weilu/Research/data/test_data/test_data_4.csv\")\n",
    "raw_test_data_2 = raw_test_data.drop_duplicates(subset=['Qw', 'Rw', \"VTotal\"])\n",
    "raw_test_data_2 = raw_test_data_2.assign(isGood=raw_test_data_2.groupby(\"Name\")[\"GDT\"].rank(ascending=False, method='first') < 50)\n",
    "raw_test_data = raw_test_data_2\n",
    "raw_data_T0784 = raw_test_data.groupby(\"Name\").get_group(\"T0784\")\n",
    "raw_data_T0792 = raw_test_data.groupby(\"Name\").get_group(\"T0792\")\n",
    "# raw_data = pd.concat([raw_data_T0784, raw_data_T0792])\n",
    "raw_data = raw_data_T0792\n",
    "\n",
    "\n",
    "# FEATURES = [\"Rw\", \"VTotal\", \"QGO\"]\n",
    "# FEATURES = [\"Rw\", \"VTotal\", \"QGO\", \"Burial\", \"Frag_Mem\", \"Water\"]\n",
    "# FEATURES = list(raw_test_data.columns[2:-3])\n",
    "FEATURES = ['Rw',\n",
    " 'VTotal',\n",
    " 'QGO',\n",
    " 'Burial',\n",
    " 'Water',\n",
    " 'Rama',\n",
    " 'DSSP',\n",
    " 'P_AP',\n",
    " 'Helix',\n",
    " 'Frag_Mem']\n",
    "# LABEL = \"Qw\"\n",
    "LABEL = \"isGood\"\n",
    "PolynomialDegree = 1\n",
    "p = 0.1\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "class RemoveFirstFrame(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, frame):\n",
    "        self.frame = frame\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.query(f\"Step % {frame} != 1\")\n",
    "    \n",
    "num_attribs = FEATURES\n",
    "cat_attribs = [LABEL]\n",
    "frame = 201\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=PolynomialDegree, include_bias=False))\n",
    "    ])\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs))\n",
    "    ])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "my_full_pipeline = Pipeline([\n",
    "#         ('removeFirstFrame', RemoveFirstFrame(frame)),\n",
    "        ('featureSelection', full_pipeline)\n",
    "])\n",
    "    \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(raw_data, raw_data[\"isGood\"]):\n",
    "    strat_train_set = raw_data.iloc[train_index]\n",
    "    strat_test_set = raw_data.iloc[test_index]\n",
    "# strat_test_set[LABEL].value_counts() / len(strat_test_set)\n",
    "X_train = my_full_pipeline.fit_transform(strat_train_set)\n",
    "X_test = my_full_pipeline.fit_transform(strat_test_set)\n",
    "train_y = X_train[:,-1]\n",
    "train_set = X_train[:,:-1]\n",
    "test_y = X_test[:,-1]\n",
    "test_set = X_test[:,:-1]\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# log_clf = LogisticRegression(random_state=142)\n",
    "# rnd_clf = RandomForestClassifier(random_state=432)\n",
    "# svm_clf = SVC(probability=True, random_state=412)\n",
    "log_clf = LogisticRegression(random_state=142, class_weight={0:p, 1:(1-p)})\n",
    "rnd_clf = RandomForestClassifier(random_state=432, class_weight={0:p, 1:(1-p)})\n",
    "svm_clf = SVC(probability=True, random_state=412, class_weight={0:p, 1:(1-p)})\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "log_clf.fit(train_set, train_y)\n",
    "rnd_clf.fit(train_set, train_y)\n",
    "svm_clf.fit(train_set, train_y)\n",
    "voting_clf.fit(train_set, train_y)\n",
    "\n",
    "\n",
    "# check on training set\n",
    "n = 10\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "#     y_pred = clf.predict(train_set)\n",
    "    prob= clf.predict_proba(train_set)[:,1]\n",
    "    position_of_top_n = prob.argsort()[-n:][::-1]\n",
    "    threshold = prob[position_of_top_n][-1]\n",
    "    predict_y = np.zeros(len(train_y),)\n",
    "    predict_y[position_of_top_n] = 1\n",
    "#     predict_y = (test > threshold)\n",
    "#     print(threshold)\n",
    "    cm = confusion_matrix(train_y, predict_y)\n",
    "#     print(clf.__class__.__name__, \"\\n\", accuracy_score(train_y, predict_y))\n",
    "    print(clf.__class__.__name__, \"\\n\", cm)\n",
    "    \n",
    "time_stamp = f\"{datetime.today().strftime('%d_%h_%H%M%S')}\"\n",
    "for name, data in raw_test_data.groupby(\"Name\"):\n",
    "    print(name)\n",
    "    X = full_pipeline.fit_transform(data)\n",
    "    eval_y = X[:,-1]\n",
    "    eval_set = X[:,:-1]\n",
    "    test= log_clf.predict_proba(eval_set)[:,1]\n",
    "    position_of_top_n = test.argsort()[-n:][::-1]\n",
    "    threshold = test[position_of_top_n][-1]\n",
    "    predict_y = np.zeros(len(eval_y),)\n",
    "    predict_y[position_of_top_n] = 1\n",
    "\n",
    "    with open(f\"/Users/weilu/Research/data/structure_selector/p{p}_poly{PolynomialDegree}_{name}.csv\", \"w\") as f:\n",
    "        f.write(\"Result\\n\")\n",
    "        for i in test:\n",
    "            f.write(str(i) + \"\\n\")\n",
    "#     with open(f\"/Users/weilu/Research/data/structure_selector/{name}_results_{time_stamp}.csv\", \"w\") as f:\n",
    "#         f.write(\"Result\\n\")\n",
    "#         for i in test:\n",
    "#             f.write(str(i) + \"\\n\")\n",
    "\n",
    "#     predict_y = (test > threshold)\n",
    "#     print(threshold)\n",
    "    print(confusion_matrix(eval_y, predict_y))\n",
    "print(f\"p{p}_poly{PolynomialDegree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_test_data = pd.read_csv(\"/Users/weilu/Research/data/test_data/test_data_4.csv\")\n",
    "raw_test_data_2 = raw_test_data.drop_duplicates(subset=['Qw', 'Rw', \"VTotal\"])\n",
    "raw_test_data_2 = raw_test_data_2.assign(isGood=raw_test_data_2.groupby(\"Name\")[\"GDT\"].rank(ascending=False, method='first') < 50)\n",
    "raw_test_data = raw_test_data_2\n",
    "raw_data_T0784 = raw_test_data.groupby(\"Name\").get_group(\"T0784\")\n",
    "raw_data_T0792 = raw_test_data.groupby(\"Name\").get_group(\"T0792\")\n",
    "# raw_data = pd.concat([raw_data_T0784, raw_data_T0792])\n",
    "raw_data = raw_data_T0792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression \n",
      " [[1559    2]\n",
      " [  36    3]]\n",
      "1MBA\n",
      "[[1947    5]\n",
      " [  49    0]]\n",
      "T0766\n",
      "[[1947    5]\n",
      " [  49    0]]\n",
      "T0784\n",
      "[[1947    5]\n",
      " [  49    0]]\n",
      "T0792\n",
      "[[1949    3]\n",
      " [  47    2]]\n",
      "T0803\n",
      "[[1947    5]\n",
      " [  49    0]]\n",
      "T0815\n",
      "[[1949    3]\n",
      " [  47    2]]\n",
      "T0833\n",
      "[[1919    5]\n",
      " [  49    0]]\n",
      "T251\n",
      "[[1947    5]\n",
      " [  49    0]]\n",
      "p0.1_poly2\n"
     ]
    }
   ],
   "source": [
    "# FEATURES = [\"Rw\", \"VTotal\", \"QGO\"]\n",
    "# FEATURES = [\"Rw\", \"VTotal\", \"QGO\", \"Burial\", \"Frag_Mem\", \"Water\"]\n",
    "# FEATURES = list(raw_test_data.columns[2:-3])\n",
    "FEATURES = ['Rw',\n",
    " 'VTotal',\n",
    " 'QGO',\n",
    " 'Burial',\n",
    " 'Water',\n",
    " 'Rama',\n",
    " 'DSSP',\n",
    " 'P_AP',\n",
    " 'Helix',\n",
    " 'Frag_Mem']\n",
    "# LABEL = \"Qw\"\n",
    "LABEL = \"isGood\"\n",
    "PolynomialDegree = 2\n",
    "p = 0.1\n",
    "\n",
    "\n",
    "num_attribs = FEATURES\n",
    "cat_attribs = [LABEL]\n",
    "frame = 201\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=PolynomialDegree, include_bias=False))\n",
    "    ])\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs))\n",
    "    ])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "my_full_pipeline = Pipeline([\n",
    "#         ('removeFirstFrame', RemoveFirstFrame(frame)),\n",
    "        ('featureSelection', full_pipeline)\n",
    "])\n",
    "    \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(raw_data, raw_data[\"isGood\"]):\n",
    "    strat_train_set = raw_data.iloc[train_index]\n",
    "    strat_test_set = raw_data.iloc[test_index]\n",
    "# strat_test_set[LABEL].value_counts() / len(strat_test_set)\n",
    "X_train = my_full_pipeline.fit_transform(strat_train_set)\n",
    "X_test = my_full_pipeline.fit_transform(strat_test_set)\n",
    "train_y = X_train[:,-1]\n",
    "train_set = X_train[:,:-1]\n",
    "test_y = X_test[:,-1]\n",
    "test_set = X_test[:,:-1]\n",
    "\n",
    "\n",
    "\n",
    "# log_clf = LogisticRegression(random_state=142)\n",
    "# rnd_clf = RandomForestClassifier(random_state=432)\n",
    "# svm_clf = SVC(probability=True, random_state=412)\n",
    "log_clf = LogisticRegression(random_state=142, class_weight={0:p, 1:(1-p)})\n",
    "\n",
    "log_clf.fit(train_set, train_y)\n",
    "\n",
    "\n",
    "# check on training set\n",
    "n = 5\n",
    "\n",
    "clf = log_clf\n",
    "#     y_pred = clf.predict(train_set)\n",
    "prob= clf.predict_proba(train_set)[:,1]\n",
    "position_of_top_n = prob.argsort()[-n:][::-1]\n",
    "threshold = prob[position_of_top_n][-1]\n",
    "predict_y = np.zeros(len(train_y),)\n",
    "predict_y[position_of_top_n] = 1\n",
    "#     predict_y = (test > threshold)\n",
    "#     print(threshold)\n",
    "cm = confusion_matrix(train_y, predict_y)\n",
    "#     print(clf.__class__.__name__, \"\\n\", accuracy_score(train_y, predict_y))\n",
    "print(clf.__class__.__name__, \"\\n\", cm)\n",
    "\n",
    "\n",
    "\n",
    "for name, data in raw_test_data.groupby(\"Name\"):\n",
    "    print(name)\n",
    "    X = full_pipeline.fit_transform(data)\n",
    "    eval_y = X[:,-1]\n",
    "    eval_set = X[:,:-1]\n",
    "    test= log_clf.predict_proba(eval_set)[:,1]\n",
    "    position_of_top_n = test.argsort()[-n:][::-1]\n",
    "    threshold = test[position_of_top_n][-1]\n",
    "    predict_y = np.zeros(len(eval_y),)\n",
    "    predict_y[position_of_top_n] = 1\n",
    "    print(confusion_matrix(eval_y, predict_y))\n",
    "print(f\"p{p}_poly{PolynomialDegree}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weilu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "strat_train_set[\"a\"] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x107a0e978>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAERCAYAAABhKjCtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1078d8dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "strat_train_set.plot(\"a\", \"GDT\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.16491821e-11,   3.72178125e-02,   1.06612780e-01, ...,\n",
       "         4.89392979e-03,   2.67241498e-05,   2.70575860e-03])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Step</th>\n",
       "      <th>Qw</th>\n",
       "      <th>Rw</th>\n",
       "      <th>VTotal</th>\n",
       "      <th>QGO</th>\n",
       "      <th>Burial</th>\n",
       "      <th>Water</th>\n",
       "      <th>Rama</th>\n",
       "      <th>Chain</th>\n",
       "      <th>Chi</th>\n",
       "      <th>DSSP</th>\n",
       "      <th>P_AP</th>\n",
       "      <th>Helix</th>\n",
       "      <th>Frag_Mem</th>\n",
       "      <th>GDT</th>\n",
       "      <th>Name</th>\n",
       "      <th>Good</th>\n",
       "      <th>isGood</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.164918e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.721781e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.066128e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.563204e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.127779e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.576430e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.272314e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.276116e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.763054e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.304295e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.630703e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.785011e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.076388e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.518153e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.209818e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.666894e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.515937e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.976814e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.768104e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.341261e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.703344e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.648333e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.073082e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.661211e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.989559e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.778049e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.146910e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.057560e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.648288e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.149428e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8006</th>\n",
       "      <td>1977.0</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>-10593.311770</td>\n",
       "      <td>-512.049764</td>\n",
       "      <td>20.236916</td>\n",
       "      <td>-67.812683</td>\n",
       "      <td>-22.184711</td>\n",
       "      <td>-278.062233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.095452</td>\n",
       "      <td>-5.979070</td>\n",
       "      <td>-8.054282</td>\n",
       "      <td>-137.098250</td>\n",
       "      <td>75.0000</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8008</th>\n",
       "      <td>1979.0</td>\n",
       "      <td>0.658984</td>\n",
       "      <td>-10910.168623</td>\n",
       "      <td>-516.043438</td>\n",
       "      <td>20.959818</td>\n",
       "      <td>-67.384450</td>\n",
       "      <td>-27.034083</td>\n",
       "      <td>-283.032187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.337310</td>\n",
       "      <td>-5.914448</td>\n",
       "      <td>-14.333326</td>\n",
       "      <td>-128.967451</td>\n",
       "      <td>71.8750</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8009</th>\n",
       "      <td>1980.0</td>\n",
       "      <td>0.669392</td>\n",
       "      <td>-10808.838737</td>\n",
       "      <td>-510.685731</td>\n",
       "      <td>20.174154</td>\n",
       "      <td>-69.985929</td>\n",
       "      <td>-26.759350</td>\n",
       "      <td>-273.270986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.131982</td>\n",
       "      <td>-5.751354</td>\n",
       "      <td>-12.168164</td>\n",
       "      <td>-131.792120</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8011</th>\n",
       "      <td>1982.0</td>\n",
       "      <td>0.650550</td>\n",
       "      <td>-10710.248358</td>\n",
       "      <td>-503.013859</td>\n",
       "      <td>18.053729</td>\n",
       "      <td>-66.983437</td>\n",
       "      <td>-25.137358</td>\n",
       "      <td>-270.321328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.017909</td>\n",
       "      <td>-5.530218</td>\n",
       "      <td>-12.006409</td>\n",
       "      <td>-129.070929</td>\n",
       "      <td>66.2500</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8012</th>\n",
       "      <td>1983.0</td>\n",
       "      <td>0.685214</td>\n",
       "      <td>-10691.855763</td>\n",
       "      <td>-522.016160</td>\n",
       "      <td>20.236039</td>\n",
       "      <td>-68.275471</td>\n",
       "      <td>-28.011384</td>\n",
       "      <td>-286.631050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.535751</td>\n",
       "      <td>-6.087053</td>\n",
       "      <td>-14.097049</td>\n",
       "      <td>-126.614441</td>\n",
       "      <td>73.4375</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8013</th>\n",
       "      <td>1984.0</td>\n",
       "      <td>0.665166</td>\n",
       "      <td>-10574.798646</td>\n",
       "      <td>-502.934013</td>\n",
       "      <td>18.294475</td>\n",
       "      <td>-66.177617</td>\n",
       "      <td>-31.415967</td>\n",
       "      <td>-269.281665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.208359</td>\n",
       "      <td>-5.915740</td>\n",
       "      <td>-11.716979</td>\n",
       "      <td>-125.512160</td>\n",
       "      <td>70.6250</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8014</th>\n",
       "      <td>1985.0</td>\n",
       "      <td>0.648859</td>\n",
       "      <td>-10533.233444</td>\n",
       "      <td>-498.100380</td>\n",
       "      <td>19.709472</td>\n",
       "      <td>-67.679027</td>\n",
       "      <td>-26.093886</td>\n",
       "      <td>-271.551445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.095048</td>\n",
       "      <td>-5.723215</td>\n",
       "      <td>-9.105124</td>\n",
       "      <td>-126.562106</td>\n",
       "      <td>69.0625</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8015</th>\n",
       "      <td>1986.0</td>\n",
       "      <td>0.651357</td>\n",
       "      <td>-10524.377624</td>\n",
       "      <td>-510.714068</td>\n",
       "      <td>20.912523</td>\n",
       "      <td>-66.966413</td>\n",
       "      <td>-29.927654</td>\n",
       "      <td>-276.947931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.065816</td>\n",
       "      <td>-6.259854</td>\n",
       "      <td>-10.832408</td>\n",
       "      <td>-129.626516</td>\n",
       "      <td>69.6875</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8016</th>\n",
       "      <td>1987.0</td>\n",
       "      <td>0.667974</td>\n",
       "      <td>-10613.356188</td>\n",
       "      <td>-494.239741</td>\n",
       "      <td>24.187234</td>\n",
       "      <td>-67.990841</td>\n",
       "      <td>-29.886000</td>\n",
       "      <td>-264.840312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.332347</td>\n",
       "      <td>-5.533874</td>\n",
       "      <td>-9.704129</td>\n",
       "      <td>-131.139472</td>\n",
       "      <td>70.6250</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8017</th>\n",
       "      <td>1988.0</td>\n",
       "      <td>0.647765</td>\n",
       "      <td>-10474.683872</td>\n",
       "      <td>-509.541456</td>\n",
       "      <td>21.639569</td>\n",
       "      <td>-69.090910</td>\n",
       "      <td>-29.046156</td>\n",
       "      <td>-289.634132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.229456</td>\n",
       "      <td>-5.435881</td>\n",
       "      <td>-7.054904</td>\n",
       "      <td>-118.689587</td>\n",
       "      <td>69.3750</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8018</th>\n",
       "      <td>1989.0</td>\n",
       "      <td>0.681786</td>\n",
       "      <td>-10831.538327</td>\n",
       "      <td>-518.332556</td>\n",
       "      <td>19.926551</td>\n",
       "      <td>-68.044240</td>\n",
       "      <td>-26.796200</td>\n",
       "      <td>-285.541167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.556425</td>\n",
       "      <td>-5.233047</td>\n",
       "      <td>-13.082628</td>\n",
       "      <td>-127.005400</td>\n",
       "      <td>72.1875</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8019</th>\n",
       "      <td>1990.0</td>\n",
       "      <td>0.679430</td>\n",
       "      <td>-10576.888976</td>\n",
       "      <td>-522.583097</td>\n",
       "      <td>19.914406</td>\n",
       "      <td>-67.297044</td>\n",
       "      <td>-28.390955</td>\n",
       "      <td>-289.612025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.107322</td>\n",
       "      <td>-4.728231</td>\n",
       "      <td>-8.109065</td>\n",
       "      <td>-131.252860</td>\n",
       "      <td>71.8750</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8020</th>\n",
       "      <td>1991.0</td>\n",
       "      <td>0.655303</td>\n",
       "      <td>-10564.326296</td>\n",
       "      <td>-500.781869</td>\n",
       "      <td>20.554229</td>\n",
       "      <td>-68.479685</td>\n",
       "      <td>-24.964039</td>\n",
       "      <td>-278.836109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.044167</td>\n",
       "      <td>-5.667629</td>\n",
       "      <td>-5.691850</td>\n",
       "      <td>-125.652619</td>\n",
       "      <td>67.5000</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021</th>\n",
       "      <td>1992.0</td>\n",
       "      <td>0.631370</td>\n",
       "      <td>-10769.714400</td>\n",
       "      <td>-505.466108</td>\n",
       "      <td>23.076395</td>\n",
       "      <td>-68.322989</td>\n",
       "      <td>-27.142742</td>\n",
       "      <td>-275.575743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.578795</td>\n",
       "      <td>-5.285562</td>\n",
       "      <td>-8.554865</td>\n",
       "      <td>-132.081807</td>\n",
       "      <td>65.9375</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8023</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.684918</td>\n",
       "      <td>-10713.725818</td>\n",
       "      <td>-530.500090</td>\n",
       "      <td>19.317672</td>\n",
       "      <td>-68.405937</td>\n",
       "      <td>-25.224987</td>\n",
       "      <td>-290.255622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.814930</td>\n",
       "      <td>-5.638768</td>\n",
       "      <td>-14.371399</td>\n",
       "      <td>-134.106119</td>\n",
       "      <td>71.8750</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8024</th>\n",
       "      <td>1995.0</td>\n",
       "      <td>0.661095</td>\n",
       "      <td>-10625.157107</td>\n",
       "      <td>-511.080552</td>\n",
       "      <td>18.092228</td>\n",
       "      <td>-67.910231</td>\n",
       "      <td>-26.201063</td>\n",
       "      <td>-289.611421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.894607</td>\n",
       "      <td>-6.046963</td>\n",
       "      <td>-9.221339</td>\n",
       "      <td>-119.287156</td>\n",
       "      <td>69.0625</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8025</th>\n",
       "      <td>1996.0</td>\n",
       "      <td>0.638725</td>\n",
       "      <td>-10497.850915</td>\n",
       "      <td>-511.121695</td>\n",
       "      <td>20.572087</td>\n",
       "      <td>-68.852224</td>\n",
       "      <td>-23.291928</td>\n",
       "      <td>-279.488099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.650985</td>\n",
       "      <td>-5.664810</td>\n",
       "      <td>-12.633876</td>\n",
       "      <td>-129.111861</td>\n",
       "      <td>65.6250</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8026</th>\n",
       "      <td>1997.0</td>\n",
       "      <td>0.653354</td>\n",
       "      <td>-10777.222800</td>\n",
       "      <td>-506.663878</td>\n",
       "      <td>20.733806</td>\n",
       "      <td>-67.652987</td>\n",
       "      <td>-28.592670</td>\n",
       "      <td>-274.635791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.564561</td>\n",
       "      <td>-6.075437</td>\n",
       "      <td>-11.513044</td>\n",
       "      <td>-126.363193</td>\n",
       "      <td>67.5000</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8027</th>\n",
       "      <td>1998.0</td>\n",
       "      <td>0.657097</td>\n",
       "      <td>-10392.082129</td>\n",
       "      <td>-520.223713</td>\n",
       "      <td>19.196379</td>\n",
       "      <td>-68.784738</td>\n",
       "      <td>-29.643841</td>\n",
       "      <td>-290.777248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.088264</td>\n",
       "      <td>-5.129537</td>\n",
       "      <td>-9.064074</td>\n",
       "      <td>-122.932390</td>\n",
       "      <td>68.1250</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8028</th>\n",
       "      <td>1999.0</td>\n",
       "      <td>0.649594</td>\n",
       "      <td>-10693.705663</td>\n",
       "      <td>-529.215899</td>\n",
       "      <td>22.072205</td>\n",
       "      <td>-66.900385</td>\n",
       "      <td>-28.680765</td>\n",
       "      <td>-293.429028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.401104</td>\n",
       "      <td>-5.521903</td>\n",
       "      <td>-12.282255</td>\n",
       "      <td>-133.072663</td>\n",
       "      <td>69.0625</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8029</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.677115</td>\n",
       "      <td>-10707.594672</td>\n",
       "      <td>-533.335040</td>\n",
       "      <td>16.337619</td>\n",
       "      <td>-66.771902</td>\n",
       "      <td>-26.052065</td>\n",
       "      <td>-290.927237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.369208</td>\n",
       "      <td>-5.456457</td>\n",
       "      <td>-14.734835</td>\n",
       "      <td>-133.360956</td>\n",
       "      <td>72.1875</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8030</th>\n",
       "      <td>2001.0</td>\n",
       "      <td>0.679864</td>\n",
       "      <td>-10785.792271</td>\n",
       "      <td>-530.228951</td>\n",
       "      <td>18.697403</td>\n",
       "      <td>-67.118936</td>\n",
       "      <td>-26.184378</td>\n",
       "      <td>-295.350130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.742940</td>\n",
       "      <td>-5.287526</td>\n",
       "      <td>-7.008405</td>\n",
       "      <td>-135.234039</td>\n",
       "      <td>71.2500</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8031</th>\n",
       "      <td>2002.0</td>\n",
       "      <td>0.653806</td>\n",
       "      <td>-10736.275317</td>\n",
       "      <td>-523.428394</td>\n",
       "      <td>18.933004</td>\n",
       "      <td>-67.458620</td>\n",
       "      <td>-32.270373</td>\n",
       "      <td>-289.782726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.574242</td>\n",
       "      <td>-5.526367</td>\n",
       "      <td>-8.710672</td>\n",
       "      <td>-126.038398</td>\n",
       "      <td>68.7500</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8032</th>\n",
       "      <td>2003.0</td>\n",
       "      <td>0.670273</td>\n",
       "      <td>-10874.279415</td>\n",
       "      <td>-547.724272</td>\n",
       "      <td>18.656697</td>\n",
       "      <td>-69.098619</td>\n",
       "      <td>-25.463678</td>\n",
       "      <td>-302.654968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.101013</td>\n",
       "      <td>-5.431765</td>\n",
       "      <td>-14.888201</td>\n",
       "      <td>-137.742725</td>\n",
       "      <td>72.1875</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8033</th>\n",
       "      <td>2004.0</td>\n",
       "      <td>0.669436</td>\n",
       "      <td>-10892.368654</td>\n",
       "      <td>-510.008237</td>\n",
       "      <td>22.834987</td>\n",
       "      <td>-68.154433</td>\n",
       "      <td>-25.764409</td>\n",
       "      <td>-273.413226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.492297</td>\n",
       "      <td>-5.112278</td>\n",
       "      <td>-12.832885</td>\n",
       "      <td>-135.073695</td>\n",
       "      <td>72.8125</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8034</th>\n",
       "      <td>2005.0</td>\n",
       "      <td>0.656888</td>\n",
       "      <td>-10703.910853</td>\n",
       "      <td>-543.171503</td>\n",
       "      <td>17.905962</td>\n",
       "      <td>-66.398787</td>\n",
       "      <td>-29.120641</td>\n",
       "      <td>-298.888010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.132435</td>\n",
       "      <td>-5.836789</td>\n",
       "      <td>-10.355362</td>\n",
       "      <td>-137.345441</td>\n",
       "      <td>68.7500</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8035</th>\n",
       "      <td>2006.0</td>\n",
       "      <td>0.692382</td>\n",
       "      <td>-10828.222331</td>\n",
       "      <td>-534.712330</td>\n",
       "      <td>19.098412</td>\n",
       "      <td>-68.456227</td>\n",
       "      <td>-23.709948</td>\n",
       "      <td>-294.261432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.859785</td>\n",
       "      <td>-5.409702</td>\n",
       "      <td>-12.117053</td>\n",
       "      <td>-137.996595</td>\n",
       "      <td>75.3125</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8037</th>\n",
       "      <td>2008.0</td>\n",
       "      <td>0.660913</td>\n",
       "      <td>-10577.606729</td>\n",
       "      <td>-505.014066</td>\n",
       "      <td>18.977488</td>\n",
       "      <td>-67.414641</td>\n",
       "      <td>-25.320651</td>\n",
       "      <td>-282.519473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.224664</td>\n",
       "      <td>-4.776406</td>\n",
       "      <td>-9.789953</td>\n",
       "      <td>-121.945767</td>\n",
       "      <td>72.5000</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8038</th>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.652170</td>\n",
       "      <td>-10611.947525</td>\n",
       "      <td>-526.856227</td>\n",
       "      <td>13.633329</td>\n",
       "      <td>-67.234170</td>\n",
       "      <td>-22.807722</td>\n",
       "      <td>-291.414826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.001837</td>\n",
       "      <td>-4.846785</td>\n",
       "      <td>-13.016733</td>\n",
       "      <td>-130.167483</td>\n",
       "      <td>70.9375</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8039</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.649245</td>\n",
       "      <td>-10571.133114</td>\n",
       "      <td>-532.716634</td>\n",
       "      <td>17.491296</td>\n",
       "      <td>-68.178990</td>\n",
       "      <td>-22.686104</td>\n",
       "      <td>-295.800491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.015912</td>\n",
       "      <td>-5.429175</td>\n",
       "      <td>-12.869567</td>\n",
       "      <td>-133.227691</td>\n",
       "      <td>71.5625</td>\n",
       "      <td>T0792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Step        Qw            Rw      VTotal        QGO     Burial  \\\n",
       "0        NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "1        NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "2        NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "3        NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "4        NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "5        NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "6        NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "7        NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "8        NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "9        NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "10       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "11       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "12       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "13       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "14       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "15       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "16       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "17       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "18       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "19       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "20       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "21       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "22       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "23       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "24       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "25       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "26       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "27       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "28       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "29       NaN       NaN           NaN         NaN        NaN        NaN   \n",
       "...      ...       ...           ...         ...        ...        ...   \n",
       "8006  1977.0  0.673644 -10593.311770 -512.049764  20.236916 -67.812683   \n",
       "8008  1979.0  0.658984 -10910.168623 -516.043438  20.959818 -67.384450   \n",
       "8009  1980.0  0.669392 -10808.838737 -510.685731  20.174154 -69.985929   \n",
       "8011  1982.0  0.650550 -10710.248358 -503.013859  18.053729 -66.983437   \n",
       "8012  1983.0  0.685214 -10691.855763 -522.016160  20.236039 -68.275471   \n",
       "8013  1984.0  0.665166 -10574.798646 -502.934013  18.294475 -66.177617   \n",
       "8014  1985.0  0.648859 -10533.233444 -498.100380  19.709472 -67.679027   \n",
       "8015  1986.0  0.651357 -10524.377624 -510.714068  20.912523 -66.966413   \n",
       "8016  1987.0  0.667974 -10613.356188 -494.239741  24.187234 -67.990841   \n",
       "8017  1988.0  0.647765 -10474.683872 -509.541456  21.639569 -69.090910   \n",
       "8018  1989.0  0.681786 -10831.538327 -518.332556  19.926551 -68.044240   \n",
       "8019  1990.0  0.679430 -10576.888976 -522.583097  19.914406 -67.297044   \n",
       "8020  1991.0  0.655303 -10564.326296 -500.781869  20.554229 -68.479685   \n",
       "8021  1992.0  0.631370 -10769.714400 -505.466108  23.076395 -68.322989   \n",
       "8023  1994.0  0.684918 -10713.725818 -530.500090  19.317672 -68.405937   \n",
       "8024  1995.0  0.661095 -10625.157107 -511.080552  18.092228 -67.910231   \n",
       "8025  1996.0  0.638725 -10497.850915 -511.121695  20.572087 -68.852224   \n",
       "8026  1997.0  0.653354 -10777.222800 -506.663878  20.733806 -67.652987   \n",
       "8027  1998.0  0.657097 -10392.082129 -520.223713  19.196379 -68.784738   \n",
       "8028  1999.0  0.649594 -10693.705663 -529.215899  22.072205 -66.900385   \n",
       "8029  2000.0  0.677115 -10707.594672 -533.335040  16.337619 -66.771902   \n",
       "8030  2001.0  0.679864 -10785.792271 -530.228951  18.697403 -67.118936   \n",
       "8031  2002.0  0.653806 -10736.275317 -523.428394  18.933004 -67.458620   \n",
       "8032  2003.0  0.670273 -10874.279415 -547.724272  18.656697 -69.098619   \n",
       "8033  2004.0  0.669436 -10892.368654 -510.008237  22.834987 -68.154433   \n",
       "8034  2005.0  0.656888 -10703.910853 -543.171503  17.905962 -66.398787   \n",
       "8035  2006.0  0.692382 -10828.222331 -534.712330  19.098412 -68.456227   \n",
       "8037  2008.0  0.660913 -10577.606729 -505.014066  18.977488 -67.414641   \n",
       "8038  2009.0  0.652170 -10611.947525 -526.856227  13.633329 -67.234170   \n",
       "8039  2010.0  0.649245 -10571.133114 -532.716634  17.491296 -68.178990   \n",
       "\n",
       "          Water        Rama  Chain  Chi       DSSP      P_AP      Helix  \\\n",
       "0           NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "1           NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "2           NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "3           NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "4           NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "5           NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "6           NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "7           NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "8           NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "9           NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "10          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "11          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "12          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "13          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "14          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "15          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "16          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "17          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "18          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "19          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "20          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "21          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "22          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "23          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "24          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "25          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "26          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "27          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "28          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "29          NaN         NaN    NaN  NaN        NaN       NaN        NaN   \n",
       "...         ...         ...    ...  ...        ...       ...        ...   \n",
       "8006 -22.184711 -278.062233    0.0  0.0 -13.095452 -5.979070  -8.054282   \n",
       "8008 -27.034083 -283.032187    0.0  0.0 -10.337310 -5.914448 -14.333326   \n",
       "8009 -26.759350 -273.270986    0.0  0.0 -11.131982 -5.751354 -12.168164   \n",
       "8011 -25.137358 -270.321328    0.0  0.0 -12.017909 -5.530218 -12.006409   \n",
       "8012 -28.011384 -286.631050    0.0  0.0 -12.535751 -6.087053 -14.097049   \n",
       "8013 -31.415967 -269.281665    0.0  0.0 -11.208359 -5.915740 -11.716979   \n",
       "8014 -26.093886 -271.551445    0.0  0.0 -11.095048 -5.723215  -9.105124   \n",
       "8015 -29.927654 -276.947931    0.0  0.0 -11.065816 -6.259854 -10.832408   \n",
       "8016 -29.886000 -264.840312    0.0  0.0  -9.332347 -5.533874  -9.704129   \n",
       "8017 -29.046156 -289.634132    0.0  0.0 -12.229456 -5.435881  -7.054904   \n",
       "8018 -26.796200 -285.541167    0.0  0.0 -12.556425 -5.233047 -13.082628   \n",
       "8019 -28.390955 -289.612025    0.0  0.0 -13.107322 -4.728231  -8.109065   \n",
       "8020 -24.964039 -278.836109    0.0  0.0 -12.044167 -5.667629  -5.691850   \n",
       "8021 -27.142742 -275.575743    0.0  0.0 -11.578795 -5.285562  -8.554865   \n",
       "8023 -25.224987 -290.255622    0.0  0.0 -11.814930 -5.638768 -14.371399   \n",
       "8024 -26.201063 -289.611421    0.0  0.0 -10.894607 -6.046963  -9.221339   \n",
       "8025 -23.291928 -279.488099    0.0  0.0 -12.650985 -5.664810 -12.633876   \n",
       "8026 -28.592670 -274.635791    0.0  0.0 -12.564561 -6.075437 -11.513044   \n",
       "8027 -29.643841 -290.777248    0.0  0.0 -13.088264 -5.129537  -9.064074   \n",
       "8028 -28.680765 -293.429028    0.0  0.0 -11.401104 -5.521903 -12.282255   \n",
       "8029 -26.052065 -290.927237    0.0  0.0 -12.369208 -5.456457 -14.734835   \n",
       "8030 -26.184378 -295.350130    0.0  0.0 -12.742940 -5.287526  -7.008405   \n",
       "8031 -32.270373 -289.782726    0.0  0.0 -12.574242 -5.526367  -8.710672   \n",
       "8032 -25.463678 -302.654968    0.0  0.0 -11.101013 -5.431765 -14.888201   \n",
       "8033 -25.764409 -273.413226    0.0  0.0 -12.492297 -5.112278 -12.832885   \n",
       "8034 -29.120641 -298.888010    0.0  0.0 -13.132435 -5.836789 -10.355362   \n",
       "8035 -23.709948 -294.261432    0.0  0.0 -11.859785 -5.409702 -12.117053   \n",
       "8037 -25.320651 -282.519473    0.0  0.0 -12.224664 -4.776406  -9.789953   \n",
       "8038 -22.807722 -291.414826    0.0  0.0 -11.001837 -4.846785 -13.016733   \n",
       "8039 -22.686104 -295.800491    0.0  0.0 -12.015912 -5.429175 -12.869567   \n",
       "\n",
       "        Frag_Mem      GDT   Name  Good isGood             0  \n",
       "0            NaN      NaN    NaN   NaN    NaN  1.164918e-11  \n",
       "1            NaN      NaN    NaN   NaN    NaN  3.721781e-02  \n",
       "2            NaN      NaN    NaN   NaN    NaN  1.066128e-01  \n",
       "3            NaN      NaN    NaN   NaN    NaN  6.563204e-04  \n",
       "4            NaN      NaN    NaN   NaN    NaN  3.127779e-03  \n",
       "5            NaN      NaN    NaN   NaN    NaN  3.576430e-02  \n",
       "6            NaN      NaN    NaN   NaN    NaN  7.272314e-03  \n",
       "7            NaN      NaN    NaN   NaN    NaN  5.276116e-02  \n",
       "8            NaN      NaN    NaN   NaN    NaN  1.763054e-03  \n",
       "9            NaN      NaN    NaN   NaN    NaN  5.304295e-03  \n",
       "10           NaN      NaN    NaN   NaN    NaN  8.630703e-01  \n",
       "11           NaN      NaN    NaN   NaN    NaN  3.785011e-06  \n",
       "12           NaN      NaN    NaN   NaN    NaN  1.076388e-03  \n",
       "13           NaN      NaN    NaN   NaN    NaN  2.518153e-02  \n",
       "14           NaN      NaN    NaN   NaN    NaN  1.209818e-02  \n",
       "15           NaN      NaN    NaN   NaN    NaN  1.666894e-02  \n",
       "16           NaN      NaN    NaN   NaN    NaN  1.515937e-04  \n",
       "17           NaN      NaN    NaN   NaN    NaN  7.976814e-10  \n",
       "18           NaN      NaN    NaN   NaN    NaN  6.768104e-03  \n",
       "19           NaN      NaN    NaN   NaN    NaN  1.341261e-05  \n",
       "20           NaN      NaN    NaN   NaN    NaN  1.703344e-02  \n",
       "21           NaN      NaN    NaN   NaN    NaN  6.648333e-03  \n",
       "22           NaN      NaN    NaN   NaN    NaN  3.073082e-01  \n",
       "23           NaN      NaN    NaN   NaN    NaN  1.661211e-01  \n",
       "24           NaN      NaN    NaN   NaN    NaN  2.989559e-02  \n",
       "25           NaN      NaN    NaN   NaN    NaN  1.778049e-04  \n",
       "26           NaN      NaN    NaN   NaN    NaN  2.146910e-07  \n",
       "27           NaN      NaN    NaN   NaN    NaN  8.057560e-03  \n",
       "28           NaN      NaN    NaN   NaN    NaN  1.648288e-02  \n",
       "29           NaN      NaN    NaN   NaN    NaN  2.149428e-05  \n",
       "...          ...      ...    ...   ...    ...           ...  \n",
       "8006 -137.098250  75.0000  T0792   0.0  False           NaN  \n",
       "8008 -128.967451  71.8750  T0792   0.0  False           NaN  \n",
       "8009 -131.792120  70.0000  T0792   0.0  False           NaN  \n",
       "8011 -129.070929  66.2500  T0792   0.0  False           NaN  \n",
       "8012 -126.614441  73.4375  T0792   0.0  False           NaN  \n",
       "8013 -125.512160  70.6250  T0792   0.0  False           NaN  \n",
       "8014 -126.562106  69.0625  T0792   0.0  False           NaN  \n",
       "8015 -129.626516  69.6875  T0792   0.0  False           NaN  \n",
       "8016 -131.139472  70.6250  T0792   0.0  False           NaN  \n",
       "8017 -118.689587  69.3750  T0792   0.0  False           NaN  \n",
       "8018 -127.005400  72.1875  T0792   0.0  False           NaN  \n",
       "8019 -131.252860  71.8750  T0792   0.0  False           NaN  \n",
       "8020 -125.652619  67.5000  T0792   0.0  False           NaN  \n",
       "8021 -132.081807  65.9375  T0792   0.0  False           NaN  \n",
       "8023 -134.106119  71.8750  T0792   0.0  False           NaN  \n",
       "8024 -119.287156  69.0625  T0792   0.0  False           NaN  \n",
       "8025 -129.111861  65.6250  T0792   0.0  False           NaN  \n",
       "8026 -126.363193  67.5000  T0792   0.0  False           NaN  \n",
       "8027 -122.932390  68.1250  T0792   0.0  False           NaN  \n",
       "8028 -133.072663  69.0625  T0792   0.0  False           NaN  \n",
       "8029 -133.360956  72.1875  T0792   0.0  False           NaN  \n",
       "8030 -135.234039  71.2500  T0792   0.0  False           NaN  \n",
       "8031 -126.038398  68.7500  T0792   0.0  False           NaN  \n",
       "8032 -137.742725  72.1875  T0792   0.0  False           NaN  \n",
       "8033 -135.073695  72.8125  T0792   0.0  False           NaN  \n",
       "8034 -137.345441  68.7500  T0792   0.0  False           NaN  \n",
       "8035 -137.996595  75.3125  T0792   0.0  False           NaN  \n",
       "8037 -121.945767  72.5000  T0792   0.0  False           NaN  \n",
       "8038 -130.167483  70.9375  T0792   0.0  False           NaN  \n",
       "8039 -133.227691  71.5625  T0792   0.0  False           NaN  \n",
       "\n",
       "[3200 rows x 19 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([strat_train_set, pd.Series(prob)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_with_my_score_function(p=0.9, PolynomialDegree=3):\n",
    "    FEATURES = ['Rw',\n",
    "     'VTotal',\n",
    "     'QGO',\n",
    "     'Burial',\n",
    "     'Water',\n",
    "     'Rama',\n",
    "     'DSSP',\n",
    "     'P_AP',\n",
    "     'Helix',\n",
    "     'Frag_Mem']\n",
    "    # LABEL = \"Qw\"\n",
    "    LABEL = \"isGood\"\n",
    "\n",
    "    num_attribs = FEATURES\n",
    "    cat_attribs = [LABEL]\n",
    "    frame = 201\n",
    "    num_pipeline = Pipeline([\n",
    "            ('selector', DataFrameSelector(num_attribs)),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=PolynomialDegree, include_bias=False))\n",
    "        ])\n",
    "    cat_pipeline = Pipeline([\n",
    "            ('selector', DataFrameSelector(cat_attribs))\n",
    "        ])\n",
    "\n",
    "    full_pipeline = FeatureUnion(transformer_list=[\n",
    "            (\"num_pipeline\", num_pipeline),\n",
    "            (\"cat_pipeline\", cat_pipeline),\n",
    "        ])\n",
    "    my_full_pipeline = Pipeline([\n",
    "    #         ('removeFirstFrame', RemoveFirstFrame(frame)),\n",
    "            ('featureSelection', full_pipeline)\n",
    "    ])\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    for train_index, test_index in split.split(raw_data, raw_data[\"isGood\"]):\n",
    "        strat_train_set = raw_data.iloc[train_index]\n",
    "        strat_test_set = raw_data.iloc[test_index]\n",
    "    # strat_test_set[LABEL].value_counts() / len(strat_test_set)\n",
    "    X_train = my_full_pipeline.fit_transform(strat_train_set)\n",
    "    X_test = my_full_pipeline.fit_transform(strat_test_set)\n",
    "    train_y = X_train[:,-1]\n",
    "    train_set = X_train[:,:-1]\n",
    "    test_y = X_test[:,-1]\n",
    "    test_set = X_test[:,:-1]\n",
    "\n",
    "    # log_clf = LogisticRegression(random_state=142)\n",
    "    # rnd_clf = RandomForestClassifier(random_state=432)\n",
    "    # svm_clf = SVC(probability=True, random_state=412)\n",
    "    log_clf = LogisticRegression(random_state=142, class_weight={0:p, 1:(1-p)})\n",
    "    log_clf.fit(train_set, train_y)\n",
    "\n",
    "#     voting_clf.fit(train_set, train_y)\n",
    "    n = 10\n",
    "    cl_name = \"lr\"\n",
    "    clf = log_clf\n",
    "#     for cl_name, clf in (\"voting\", voting_clf):\n",
    "    my_evaluation = 1.0\n",
    "    another_evaluation = 0.0\n",
    "    for name, data in raw_test_data.groupby(\"Name\"):\n",
    "#             print(name)\n",
    "#         X = full_pipeline.fit_transform(data)\n",
    "#         validation_data, test_data = train_test_split(X, test_size=0.6, random_state=124)\n",
    "        validation_data = my_full_pipeline.fit_transform(raw_data_T0784)\n",
    "        validation_y = validation_data[:,-1]\n",
    "        validation_set = validation_data[:,:-1]\n",
    "        clf.fit(train_set, train_y)\n",
    "        test= clf.predict_proba(validation_set)[:,1]\n",
    "        position_of_top_n = test.argsort()[-n:][::-1]\n",
    "        threshold = test[position_of_top_n][-1]\n",
    "        predict_y = np.zeros(len(validation_y),)\n",
    "        predict_y[position_of_top_n] = 1\n",
    "    #     predict_y = (test > threshold)\n",
    "#         print(threshold)\n",
    "        cm = confusion_matrix(validation_y, predict_y)\n",
    "#             print(cm)\n",
    "        precision = cm[1][1] / (cm[1][1] + cm[0][1])\n",
    "#             print(name,  \" precision\", precision,end = \" \")\n",
    "        if name != \"T0766\" and name != \"T0833\":\n",
    "            my_evaluation *= precision\n",
    "            another_evaluation += precision\n",
    "#         print(\"\")\n",
    "    print(\"classifier:\", cl_name, \", p:\",p, \", degree\", PolynomialDegree, \", score\", my_evaluation, \", another score\", another_evaluation)\n",
    "    return (cl_name, p, PolynomialDegree, my_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myGridSerach():\n",
    "    p_list = [0.9, 0.8, 0.7, 0.5, 0.1]\n",
    "    degree_list = [3, 2, 1]\n",
    "#     p_list = [0.1, 0.8, 0.9, 0.95]\n",
    "#     degree_list = [1, 2, 3]\n",
    "#     p_list = [0.1, 0.15, 0.2, 0.5, 0.7, 0.8, 0.85, 0.9, 0.95]\n",
    "#     degree_list = [1, 2, 3, 4]\n",
    "    result = []\n",
    "    for p in p_list:\n",
    "        for degree in degree_list:\n",
    "            result += [compute_with_my_score_function(p, degree)]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier: lr , p: 0.9 , degree 3 , score 1e-06 , another score 0.6\n",
      "classifier: lr , p: 0.9 , degree 2 , score 0.0 , another score 0.0\n",
      "classifier: lr , p: 0.9 , degree 1 , score 0.0 , another score 0.0\n",
      "classifier: lr , p: 0.8 , degree 3 , score 1e-06 , another score 0.6\n",
      "classifier: lr , p: 0.8 , degree 2 , score 0.0 , another score 0.0\n",
      "classifier: lr , p: 0.8 , degree 1 , score 0.0 , another score 0.0\n",
      "classifier: lr , p: 0.7 , degree 3 , score 0.0 , another score 0.0\n",
      "classifier: lr , p: 0.7 , degree 2 , score 0.0 , another score 0.0\n",
      "classifier: lr , p: 0.7 , degree 1 , score 0.0 , another score 0.0\n",
      "classifier: lr , p: 0.5 , degree 3 , score 0.0 , another score 0.0\n",
      "classifier: lr , p: 0.5 , degree 2 , score 0.0 , another score 0.0\n",
      "classifier: lr , p: 0.5 , degree 1 , score 0.0 , another score 0.0\n",
      "classifier: lr , p: 0.1 , degree 3 , score 0.0 , another score 0.0\n",
      "classifier: lr , p: 0.1 , degree 2 , score 0.0 , another score 0.0\n",
      "classifier: lr , p: 0.1 , degree 1 , score 0.0 , another score 0.0\n"
     ]
    }
   ],
   "source": [
    "myGridSerach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier: lr , p: 0.1 , degree 1 , score 0.0 , another score 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('lr', 0.1, 1, 0.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_with_my_score_function(0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "class RemoveFirstFrame(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, frame):\n",
    "        self.frame = frame\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.query(f\"Step % {frame} != 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_attribs = FEATURES\n",
    "cat_attribs = [LABEL]\n",
    "frame = 201\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=PolynomialDegree, include_bias=False))\n",
    "    ])\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs))\n",
    "    ])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "my_full_pipeline = Pipeline([\n",
    "#         ('removeFirstFrame', RemoveFirstFrame(frame)),\n",
    "        ('featureSelection', full_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(raw_data, raw_data[\"isGood\"]):\n",
    "    strat_train_set = raw_data.iloc[train_index]\n",
    "    strat_test_set = raw_data.iloc[test_index]\n",
    "# strat_test_set[LABEL].value_counts() / len(strat_test_set)\n",
    "X_train = my_full_pipeline.fit_transform(strat_train_set)\n",
    "X_test = my_full_pipeline.fit_transform(strat_test_set)\n",
    "train_y = X_train[:,-1]\n",
    "train_set = X_train[:,:-1]\n",
    "test_y = X_test[:,-1]\n",
    "test_set = X_test[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight={0: 0.1, 1: 0.9}, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=142,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)), ('r...f',\n",
       "  max_iter=-1, probability=True, random_state=412, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# log_clf = LogisticRegression(random_state=142)\n",
    "# rnd_clf = RandomForestClassifier(random_state=432)\n",
    "# svm_clf = SVC(probability=True, random_state=412)\n",
    "log_clf = LogisticRegression(random_state=142, class_weight={0:p, 1:(1-p)})\n",
    "rnd_clf = RandomForestClassifier(random_state=432, class_weight={0:p, 1:(1-p)})\n",
    "svm_clf = SVC(probability=True, random_state=412, class_weight={0:p, 1:(1-p)})\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "log_clf.fit(train_set, train_y)\n",
    "rnd_clf.fit(train_set, train_y)\n",
    "svm_clf.fit(train_set, train_y)\n",
    "voting_clf.fit(train_set, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression \n",
      " [[1558    4]\n",
      " [  32    6]]\n",
      "RandomForestClassifier \n",
      " [[1562    0]\n",
      " [  28   10]]\n",
      "SVC \n",
      " [[1559    3]\n",
      " [  31    7]]\n",
      "VotingClassifier \n",
      " [[1561    1]\n",
      " [  29    9]]\n"
     ]
    }
   ],
   "source": [
    "# check on training set\n",
    "n = 10\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "#     y_pred = clf.predict(train_set)\n",
    "    prob= clf.predict_proba(train_set)[:,1]\n",
    "    position_of_top_n = prob.argsort()[-n:][::-1]\n",
    "    threshold = prob[position_of_top_n][-1]\n",
    "    predict_y = np.zeros(len(train_y),)\n",
    "    predict_y[position_of_top_n] = 1\n",
    "#     predict_y = (test > threshold)\n",
    "#     print(threshold)\n",
    "    cm = confusion_matrix(train_y, predict_y)\n",
    "#     print(clf.__class__.__name__, \"\\n\", accuracy_score(train_y, predict_y))\n",
    "    print(clf.__class__.__name__, \"\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1MBA\n",
      "[[1942    9]\n",
      " [  49    1]]\n",
      "T0766\n",
      "[[1943    9]\n",
      " [  48    1]]\n",
      "T0784\n",
      "[[1939   10]\n",
      " [  52    0]]\n",
      "T0792\n",
      "[[1950    4]\n",
      " [  41    6]]\n",
      "T0803\n",
      "[[1943    9]\n",
      " [  48    1]]\n",
      "T0815\n",
      "[[1945    9]\n",
      " [  46    1]]\n",
      "T0833\n",
      "[[1914   10]\n",
      " [  49    0]]\n",
      "T251\n",
      "[[1940   10]\n",
      " [  51    0]]\n"
     ]
    }
   ],
   "source": [
    "time_stamp = f\"{datetime.today().strftime('%d_%h_%H%M%S')}\"\n",
    "for name, data in raw_test_data.groupby(\"Name\"):\n",
    "    print(name)\n",
    "    X = full_pipeline.fit_transform(data)\n",
    "    eval_y = X[:,-1]\n",
    "    eval_set = X[:,:-1]\n",
    "    test= log_clf.predict_proba(eval_set)[:,1]\n",
    "    position_of_top_n = test.argsort()[-n:][::-1]\n",
    "    threshold = test[position_of_top_n][-1]\n",
    "    predict_y = np.zeros(len(eval_y),)\n",
    "    predict_y[position_of_top_n] = 1\n",
    "\n",
    "    with open(f\"/Users/weilu/Research/data/structure_selector/p{p}_poly{PolynomialDegree}_{name}.csv\", \"w\") as f:\n",
    "        f.write(\"Result\\n\")\n",
    "        for i in test:\n",
    "            f.write(str(i) + \"\\n\")\n",
    "#     with open(f\"/Users/weilu/Research/data/structure_selector/{name}_results_{time_stamp}.csv\", \"w\") as f:\n",
    "#         f.write(\"Result\\n\")\n",
    "#         for i in test:\n",
    "#             f.write(str(i) + \"\\n\")\n",
    "\n",
    "#     predict_y = (test > threshold)\n",
    "#     print(threshold)\n",
    "    print(confusion_matrix(eval_y, predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1MBA\n",
      "[[1943    8]\n",
      " [  48    2]]\n",
      "T0766\n",
      "[[1946    6]\n",
      " [  45    4]]\n",
      "T0784\n",
      "[[1939   10]\n",
      " [  52    0]]\n",
      "T0792\n",
      "[[1949    5]\n",
      " [  42    5]]\n",
      "T0803\n",
      "[[1944    8]\n",
      " [  47    2]]\n",
      "T0815\n",
      "[[1946    8]\n",
      " [  45    2]]\n",
      "T0833\n",
      "[[1915    9]\n",
      " [  48    1]]\n",
      "T251\n",
      "[[1941    9]\n",
      " [  50    1]]\n"
     ]
    }
   ],
   "source": [
    "time_stamp = f\"{datetime.today().strftime('%d_%h_%H%M%S')}\"\n",
    "for name, data in raw_test_data.groupby(\"Name\"):\n",
    "    print(name)\n",
    "    X = my_full_pipeline.fit_transform(data)\n",
    "    eval_y = X[:,-1]\n",
    "    eval_set = X[:,:-1]\n",
    "    test= log_clf.predict_proba(eval_set)[:,1]\n",
    "    position_of_top_n = test.argsort()[-n:][::-1]\n",
    "    threshold = test[position_of_top_n][-1]\n",
    "    predict_y = np.zeros(len(eval_y),)\n",
    "    predict_y[position_of_top_n] = 1\n",
    "\n",
    "\n",
    "#     predict_y = (test > threshold)\n",
    "#     print(threshold)\n",
    "    print(confusion_matrix(eval_y, predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
